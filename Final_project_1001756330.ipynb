{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shravani singam- 1001756330"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below retrieves the information from the links provided and displays the stock information requested by user.\n",
    "\n",
    "Please run below cell and wait for few minitues for the complete code to run as its taking time to load the data. \n",
    "\n",
    "Step1: Using beautiful soup and lxml parser scrapped data of most actives,gainers and losers from the url given.\n",
    "\n",
    "step2: Again using beautiful soup and lxml parser scrapped the data of todays open,prev, market and volume and stored it into a dict.\n",
    "\n",
    "step3: Created a csv file called stocks.csv and stored all the above information into that file\n",
    "\n",
    "step4: Asked the user for the input and gave the information for that paticular ticker from the csv. \n",
    " \n",
    "Expected Output: \n",
    "\n",
    "This is a program to scrape data from the https://money.cnn.com/data/hotstocks/ for a class project. \n",
    "\n",
    "Which stock are you interested in: \n",
    "\n",
    "Most Actives:\n",
    "TWTR Twitter Inc\n",
    "GE General Electric Co\n",
    "F Ford Motor Co\n",
    "PFE Pfizer Inc\n",
    "MRO Marathon Oil Corp\n",
    "BAC Bank of America Corp\n",
    "T AT&T Inc\n",
    "KMI Kinder Morgan Inc\n",
    "FCX Freeport-McMoRan Inc\n",
    "CCL Carnival Corp\n",
    "\n",
    "Gainers:\n",
    "BKR Baker Hughes Co\n",
    "MHK Mohawk Industries Inc\n",
    "GPS Gap Inc\n",
    "NOV Nov Inc\n",
    "HFC HollyFrontier Corp\n",
    "VLO Valero Energy Corp\n",
    "HAL Halliburton Co\n",
    "FDX FedEx Corp\n",
    "SLB Schlumberger NV\n",
    "NUE Nucor Corp\n",
    "\n",
    "\n",
    "Losers:\n",
    "EL Estee Lauder Companies Inc\n",
    "BIO Bio Rad Laboratories Inc\n",
    "NOW ServiceNow Inc\n",
    "CRM Salesforce.Com Inc\n",
    "KEYS Keysight Technologies Inc\n",
    "IR Ingersoll Rand Inc\n",
    "EXR Extra Space Storage Inc\n",
    "ICE Intercontinental Exchange Inc\n",
    "RE Everest Re Group Ltd\n",
    "DLR Digital Realty Trust Inc\n",
    "userinputs:EL\n",
    " \n",
    "The data for EL Estee Lauder Companies Inc is the following:\n",
    " \n",
    "EL Estee Lauder Companies Inc\n",
    "PREV CLOSE:313.80\n",
    "OPEN:302.50\n",
    "VOLUME:5,039,327\n",
    "MARKET CAP:$113.8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a program to scrape data from the https://money.cnn.com/data/hotstocks/ for a class project.\n",
      "\n",
      "Which stock are you interested in:\n",
      "\n",
      "Most Actives:\n",
      "GE General Electric Co\n",
      "FCX Freeport-McMoRan Inc\n",
      "F Ford Motor Co\n",
      "BAC Bank of America Corp\n",
      "T AT&T Inc\n",
      "XOM Exxon Mobil Corp\n",
      "PFE Pfizer Inc\n",
      "WFC Wells Fargo & Co\n",
      "VZ Verizon Communications Inc\n",
      "MRO Marathon Oil Corp\n",
      " \n",
      " \n",
      "Gainers:\n",
      "LYV Live Nation Entertainment Inc\n",
      "BDX Becton Dickinson and Co\n",
      "KR Kroger Co\n",
      "DUK Duke Energy Corp\n",
      "BBY Best Buy Co Inc\n",
      "WU Western Union Co\n",
      "CLX Clorox Co\n",
      "NUE Nucor Corp\n",
      "KMI Kinder Morgan Inc\n",
      "MCK Mckesson Corp\n",
      " \n",
      " \n",
      "Losers:\n",
      "GNRC Generac Holdings Inc\n",
      "UAA Under Armour Inc\n",
      "UA Under Armour Inc\n",
      "TPR Tapestry Inc\n",
      "MOS Mosaic Co\n",
      " \n",
      " \n",
      "Writing the data into csv file.Please wait:\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# import urllib.request, urllib.parse, urllib.error\n",
    "import urllib.request,urllib.parse,urllib.error\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import pandas as pd\n",
    "import csv\n",
    "import ssl\n",
    "\n",
    "#creating a secure socket connection\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "#creating a handle to scrape data from the given link\n",
    "url = 'https://money.cnn.com/data/hotstocks/'\n",
    "html = urllib.request.urlopen(url,context = ctx).read()\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "header= soup.find('h3',text='Most Actives')\n",
    "header_table = header.find_next_sibling('table')\n",
    "table_rows = header_table.find_all('tr')\n",
    "category_list=[]\n",
    "ticker_list=[]\n",
    "title_list=[]\n",
    "new_dict={}\n",
    "print('This is a program to scrape data from the https://money.cnn.com/data/hotstocks/ for a class project.\\n')\n",
    "print('Which stock are you interested in:')\n",
    "print('')\n",
    "print('Most Actives:')\n",
    "for tr in table_rows: # from every row of most actives table, priting all companies in Most actives\n",
    "    if tr.find('td')!= None:\n",
    "        td = tr.find('td')\n",
    "        print(td.text) #printing all the companies under Most Actives\n",
    "        category_list.append(\"Most Actives\")\n",
    "        ticker_symbol = td.a.text # ticker symbol of each company is being scraped\n",
    "        ticker_link = td.a.get(\"href\")\n",
    "        ticker_list.append(ticker_symbol) # all the ticker symbols are appended into ticker_list\n",
    "        ticker_title = td.span.get('title')\n",
    "        title_list.append(ticker_title) # all ticker titles are appended into the title_list\n",
    "        new_dict[ticker_symbol]=ticker_title # ticker_title and ticker symbols are added into a dictionary new_dict \n",
    "print(\" \")\n",
    "print(\" \")\n",
    "header1= soup.find('h3',text='Gainers')\n",
    "header_table1 = header1.find_next_sibling('table') # finding the Gainers table from the website\n",
    "table_rows1 = header_table1.find_all('tr')\n",
    "print('Gainers:')\n",
    "for tr in table_rows1: # from every row of most actives table, priting all companies in Gainers\n",
    "    if tr.find('td')!= None:\n",
    "        td = tr.find('td')\n",
    "        print(td.text) #printing all the companies under Gainers\n",
    "        category_list.append(\"Gainers\")\n",
    "        ticker_symbol = td.a.text\n",
    "        ticker_link = td.a.get(\"href\")\n",
    "        ticker_list.append(ticker_symbol)  # all the ticker symbols are appended into ticker_list\n",
    "        ticker_title = td.span.get('title')\n",
    "        title_list.append(ticker_title)# all ticker titles are appended into the title_list\n",
    "        new_dict[ticker_symbol]=ticker_title\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "header2= soup.find('h3',text='Losers')\n",
    "header_table2 = header2.find_next_sibling('table') # finding the Losers table from the website\n",
    "table_rows2 = header_table2.find_all('tr')\n",
    "print('Losers:')\n",
    "for tr in table_rows2:# from every row of most actives table, priting all companies in Losers\n",
    "    if tr.find('td')!= None:\n",
    "        td = tr.find('td')\n",
    "        print(td.text)#printing all the companies under Losers\n",
    "        category_list.append('Losers')\n",
    "        ticker_symbol = td.a.text\n",
    "        ticker_link = td.a.get(\"href\")\n",
    "        ticker_list.append(ticker_symbol)# all the ticker symbols are appended into ticker_list\n",
    "        ticker_title = td.span.get('title')\n",
    "        title_list.append(ticker_title) # all ticker titles are appended into the title_list\n",
    "        new_dict[ticker_symbol]=ticker_title\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Writing the data into csv file.Please wait:\")\n",
    "\n",
    "ge_dict={}\n",
    "a_list=[]\n",
    "for i in range(len(ticker_list)): # writing this loop to load all the required values into csv file\n",
    "    cnnlink_l = 'https://money.cnn.com/quote/quote.html?symb={ticker}'.format(ticker = ticker_list[i]) #for each i , new company website is opened\n",
    "    html1_ge_l = urllib.request.urlopen(cnnlink_l,context = ctx).read()\n",
    "    soup1_ge_l = BeautifulSoup(html1_ge_l,'lxml')\n",
    "    contentTable_l = soup1_ge_l.find('table', { \"class\" : \"wsod_dataTable wsod_dataTableBig\"})#scraping the table with previous close,open etc. values\n",
    "    rows_l  = contentTable_l.find_all('tr')\n",
    "    for row in rows_l: # repeats through all the rows and scrapes the names and their values.\n",
    "        if row.find('td')!= None:\n",
    "            td = row.find('td')\n",
    "            td1 = row.find('td',{\"class\":\"wsod_quoteDataPoint\"})\n",
    "            text1 = td1.text\n",
    "            ge_dict[td.text]=text1 # text1 has the values and td.text has their names open,previous close etc and writing them into a dictionary\n",
    "            dictionary_copy = ge_dict.copy()# creating a copy of the dictionary created above\n",
    "    a_list.append(dictionary_copy) # appending all these dictionaries to a_list\n",
    "\n",
    "# Creating a file and storing all the information       \n",
    "with open(\"nati.csv\", \"w\") as stocksfile:\n",
    "    stocksfile = csv.writer(stocksfile)\n",
    "    stocksfile.writerow([\"category\",\"Symbol\", \"Name\",\"Previous close\",\"Todays open\",\"Volume\",\"Market Cap\"]) # adding a header row\n",
    "    for i in range(len(ticker_list)):\n",
    "        stocksfile.writerow([category_list[i],ticker_list[i],title_list[i],a_list[i]['Previous close'],a_list[i]['Todayâ€™s open'],a_list[i]['Volume'],a_list[i]['Market cap']])\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "#The below code takes the user input and returns the details of the company based on the user input\n",
    "while True: #This while loop is written so that if the user gives a wrong input then this gives another chance for the user to enter.\n",
    "    userinput = input(\"userinputs:\").upper() \n",
    "    if userinput not in ticker_list: # if user input is not in the companies displayed\n",
    "        print(\"Please enter the input only from the above companies\")\n",
    "        continue #continues the while loop \n",
    "    else: # if user input is in the ticker_list then executes the below code\n",
    "        cnnlink = 'https://money.cnn.com/quote/quote.html?symb={ticker}'.format(ticker = userinput) #directs to the url based on the userinput\n",
    "        html1_ge = urllib.request.urlopen(cnnlink,context = ctx).read()\n",
    "        soup1_ge = BeautifulSoup(html1_ge,'lxml')\n",
    "        contentTable  = soup1_ge.find('table', { \"class\" : \"wsod_dataTable wsod_dataTableBig\"})\n",
    "        u_dict={}\n",
    "        rows  = contentTable.find_all('tr')\n",
    "        for row in rows:\n",
    "            if row.find('td')!= None:\n",
    "                td = row.find('td')\n",
    "                td1 = row.find('td',{\"class\":\"wsod_quoteDataPoint\"})\n",
    "                text1 = td1.text\n",
    "                u_dict[td.text]=text1 # u_dict contains keys as previous close, open, volume etc. and Values as their values.\n",
    "        print(\" \")\n",
    "        print(\"The data for {0} {1} is the following:\".format(userinput,new_dict[userinput]))\n",
    "        print(\" \")\n",
    "        print(userinput,new_dict[userinput])\n",
    "        print(\"PREV CLOSE:{0}\".format(u_dict['Previous close'])) # printing the values of the Previous close of the given company.\n",
    "        print(\"OPEN:{0}\".format(u_dict['Todayâ€™s open']))\n",
    "        print(\"VOLUME:{0}\".format(u_dict['Volume']))\n",
    "        print(\"MARKET CAP:{0}\".format(u_dict['Market cap']))\n",
    "        break # breaks the loop once the user gets details about a company , this break should be skipped if the user want to know about multiple companies in one run of the program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
